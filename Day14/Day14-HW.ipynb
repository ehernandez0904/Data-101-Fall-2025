{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce44393-3036-4c90-904f-1e85718f57fb",
   "metadata": {},
   "source": [
    "## Introduction to Data Science\n",
    "\n",
    "#### University of Redlands - DATA 101\n",
    "#### Prof: Joanna Bieri [joanna_bieri@redlands.edu](mailto:joanna_bieri@redlands.edu)\n",
    "#### [Class Website: data101.joannabieri.com](https://joannabieri.com/data101.html)\n",
    "\n",
    "---------------------------------------\n",
    "# Homework Day 14\n",
    "---------------------------------------\n",
    "\n",
    "GOALS:\n",
    "\n",
    "1. Reflect on Algorithmic bias\n",
    "2. Consider your role in Data Ethics\n",
    "3. Report on your reading.\n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "This homework has **3 questions** and **1 reading report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c422e3-32b1-452f-89a8-2586784d3957",
   "metadata": {},
   "source": [
    "## Important Information\n",
    "\n",
    "- Email: [joanna_bieri@redlands.edu](mailto:joanna_bieri@redlands.edu)\n",
    "- Office Hours: Duke 209 <a href=\"https://joannabieri.com/schedule.html\"> Click Here for Joanna's Schedule</a>\n",
    "\n",
    "## If you start having trouble with git!!!\n",
    "\n",
    "Some people have reported that GIT is disappearing or giving errors on when they try to use it in Jupyter Lab. Here is another option for interacting with git:\n",
    "\n",
    "[Git Desktop](https://github.com/apps/desktop)\n",
    "\n",
    "If yous start having errors, try downloading this app. I can show you how to use it in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd0fc5-0e82-4265-a328-e906366c4907",
   "metadata": {},
   "source": [
    "## Report on your Data Ethics reading:\n",
    "\n",
    "**Your answers should be written as neatly as possible in Markdown cells**\n",
    "\n",
    "Your homework for today is all essay and written work. Make sure you respond to the three questions in the lecture:\n",
    "\n",
    "**Q1** What is your response to our discussion of bias in algorithms? Talk about the pluses and minuses of using algorithms to make decisions in our human world.\n",
    "\n",
    "**Q2**\n",
    "How do you train yourself to make the right decisions (or reduce the likelihood of accidentally making the wrong decisions) at those points?\n",
    "\n",
    "**Q3**\n",
    "How do you respond when you see bias in someones work? How could you take action to educate others?\n",
    "\n",
    "\n",
    "**Reading Report**\n",
    "\n",
    "Write a report about what you learned from your ethics reading exploration. For each book/article you read:\n",
    "\n",
    "1. Include a full proper reference to the book/article.\n",
    "   * BOOK: Author last name, First name. Book Title: Subtitle. Edition, Publisher, Year.\n",
    "   * ONLINE ARTICLE: Author last name, First name. Article Title. Website name, date accessed. html link.\n",
    "   * [MLA styles for citing other types of online work](https://style.mla.org/works-cited/citations-by-format/online-works/?gad_source=1)\n",
    "2. Write a summary in your own words what the book/article was about. Imagine telling your classmates about what they would learn by reading the article.\n",
    "3. Discuss your own reaction to the book/article. Did it have any effect on how you think about data and ethics? Do you agree with the author? What specific ideas really stood out to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53176f-7926-4d4e-967c-5b5bbd011abe",
   "metadata": {},
   "source": [
    "**Q1** I think the puluses about using algorithms to make decisions in our human world is that we can properly see different data and different opinions being displayed. I think the cons is that certain algorithms could come from certain biases and start making conclusions about a specific group of people without truly knowing who they are and either how they're culture, or lives consist of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a527f22-684c-4aee-aa4d-afd435a9c996",
   "metadata": {},
   "source": [
    "**Q2** I think training myself by getting a second view or opinion on the data I am presenting or the work I want to put out. Having a second opinion really allows for different perspectives to let me know whether or not the data is being put out there to target a group of people or if it actually contains no bias in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335bfa0-82c0-46d7-9734-f4300dedfa54",
   "metadata": {},
   "source": [
    "**Q3** I think if I see bias in someones work, I'd let them know. I say this because, if it were me displaying work that had bias in it, I would want to know before I put it out. I wouldn't want my work to affect any type of person in a negative way. I think in this world with all the technology, we could use our social media platforms to inform people about what bias actually is and how that affects people. Not just people but our society as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca89bb8-e1ea-4af6-aea6-faf8874deaf2",
   "metadata": {},
   "source": [
    "**Reading Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb2105-7678-4935-a939-18c72eca2edb",
   "metadata": {},
   "source": [
    "READING REPORT 1\n",
    "\n",
    "ARTICLE: \n",
    "\n",
    "Mitchell, Taylor. \"Algorithmic Bias in Health Care Exaberates Social Inequalities -- How to Prevent it\". harvard.edu, 22 Oct 2025. https://hsph.harvard.edu/exec-ed/news/algorithmic-bias-in-health-care-exacerbates-social-inequities-how-to-prevent-it/ . \n",
    "\n",
    "SUMMARY: \n",
    "\n",
    "The article explains how algorithms used in health care can unintentionally worsen social and racial inequities. These biases happen when algorithms are trained on data that reflect existing inequalities. For example, when certain groups are underrepresented in medical research or have less access to care. As a result, the algorithms may work well for some populations but poorly for others.\n",
    "One major example described is an algorithm used in U.S. hospitals that relied on health care spending to predict patient needs. Because Black patients or other groups of people of color, often spend less on heatlh care due to unequal access, the algorithm wrongly assumed they were healthier and gave them lower priority for special treatment programs.\n",
    "The article emphasizes that bias can appear at many stages, from data collection to model design. It calls for algorithms to be built with fairness and inclusion in mind, so they reduce rather than reinforce health disparities.\n",
    "\n",
    "MY OPINION:\n",
    "\n",
    "Yes, this article made me think more critically about how data and ethics are connected. I used to assume that algorithms were neutral, but the example of the biased health care algorithm showed that data often reflect and almost mirror existing social inequalities. I agree with the author that bias isn’t just a technical problem, it’s also an ethical one that involves whose needs are valued and who benefits from the system.\n",
    "What really stood out to me was the idea that even when race isn’t directly used in an algorithm, bias can still appear through indirect factors like income or zip code. I also found it powerful that the article calls for diverse teams and constant monitoring to make sure technology promotes fairness instead of reinforcing and more like supporting societal inequities. It made me realize that building ethical algorithms means paying attention to real world context, not just numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7cfc3-9c7f-41a4-b039-da8c516cfef0",
   "metadata": {},
   "source": [
    "READING REPORT 2\n",
    "\n",
    "ARTICLE:\n",
    "\n",
    "\"Health Care Bias is Dangerous. But So Are 'Fairness' Algorithms\". wired.com. 22 Oct 2025. https://www.wired.com/story/bias-statistics-artificial-intelligence-healthcare/ . \n",
    "\n",
    "SUMMARY:\n",
    "\n",
    "The article examines how AI and algorithmic systems used in health care carry risks of bias, not only because they reflect historic inequalities, but also because “fairness” fixes can sometimes make things worse. It starts by pointing out some concrete cases like: diagnostic tools and triage systems that perform worse for people of colour (for example, devices over estimating blood oxygen levels in darker skinned patients) and health care risk prediction tools that use spending to under estimate the needs of less resourced groups. \n",
    "The authors argue that many so called “fairness” algorithms focus mainly on matching mathematical performance metrics. While well intentioned, this approach can lead to “leveling down” or making better off groups worse off rather than lifting up disadvantaged groups and thereby ignoring the real welfare outcomes. \n",
    "They propose that a more morally acceptable path is “leveling up”: improving the algorithm’s performance for disadvantaged groups rather than forcing equality by degrading performance elsewhere. To do this effectively requires more than math: it demands better access to health care, more diverse data sets and redesigning systems from the ground up.\n",
    "\n",
    "MY OPINION:\n",
    "\n",
    "Reading this article changed the way I think about data and ethics. I used to believe that algorithms were objective (as I said before), but now I understand that they can reflect and even worsen real world inequalities if the data behind them are biased. I agree with the author that fairness in health care isn’t just about making numbers look equal but it’s about improving outcomes for everyone, especially underrepresented groups. What really stood out to me was the idea of “leveling up” instead of “leveling down,” meaning we should focus on raising the quality of care for disadvantaged groups rather than lowering it for others. It made me realize that creating fair algorithms requires not just technical skills but also empathy and awareness of social issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6faae4b-ce39-4695-a87f-99e40bda1322",
   "metadata": {},
   "source": [
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "## Further watching\n",
    "\n",
    "If you have time, really explore the world of data ethics. You could watch some of the videos linked from class.\n",
    "\n",
    "### Weapons of Math Destruction | Cathy O'Neil | Talks at Google\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=TQHs8SA1qpk >}}\n",
    "\n",
    "### Imagining a Future Free from the Algorithms of Oppression | Safiya Noble | ACL 2019\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=tNi_U1Bb1S0 >}}\n",
    "\n",
    "### Whats An Algorithm Got To Do With It\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=5zxDwA99soA >}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75adfae-6e62-4549-9856-bf7030620521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
